{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "#\n",
    "# Copyright 2017 Carlos Alberto Chavez\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of this\n",
    "# software and associated documentation files (the \"Software\"), to deal in the Software\n",
    "# without restriction, including without limitation the rights to use, copy, modify,\n",
    "# merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\n",
    "# permit persons to whom the Software is furnished to do so, subject to the following\n",
    "# conditions:\n",
    "t\n",
    "# The above copyright notice and this permission notice shall be included in all copies\n",
    "# or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\n",
    "# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\n",
    "# PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n",
    "# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n",
    "# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n",
    "# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "#os.environ[\"PATH\"] += os.pathsep + '/Users/suchethapanduranga/anaconda3/lib/python3.6/site-packages/graphviz/bin'\n",
    "\n",
    "# Uncomment these lines to use THEANO as the backend instead of tensorflow\n",
    "# # Use Theano\n",
    "# import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"theano\"\n",
    "# import keras; import keras.backend\n",
    "# if keras.backend.backend() != 'theano':\n",
    "#     raise BaseException(\"This script uses other backend\")\n",
    "# else:\n",
    "#     keras.backend.set_image_dim_ordering('th')\n",
    "#     print(\"Backend ok\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, SimpleRNN, Dense\n",
    "from keras.utils import plot_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import export_graphviz  # with pydot\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===============================\n",
    "# Hyperparameters\n",
    "# ===============================\n",
    "\n",
    "train_size_percentage = 0.82  # Training size\n",
    "mutation_rate = 0.1  # Mutation rate for GA\n",
    "min_mutation_momentum = 0.0001  # Min mutation momentum\n",
    "max_mutation_momentum = 0.1  # Max mutation momentum\n",
    "min_population = 1  # Min population for GA\n",
    "max_population = 10  # Max population for GA\n",
    "num_Iterations = 1  # Number of iterations to evaluate GA\n",
    "look_back = 1  # Num of timespaces to look back for training and testing\n",
    "max_dropout = 0.1  # Maximum percentage of dropout\n",
    "min_num_layers = 50  # Min number of hidden layers\n",
    "max_num_layers = 100 # Max number of hidden layers\n",
    "min_num_neurons = 10  # Min number of neurons in h4idden layers\n",
    "max_num_neurons = 100  # Max number of neurons in hidden layers\n",
    "min_num_estimators = 1  # Min number of random forest trees\n",
    "max_num_estimators = 2  # Max number of random forest trees\n",
    "force_gc = True  # Forces garbage collector\n",
    "rnn_epochs = 1  # Epochs for RNN\n",
    "\n",
    "# ===============================\n",
    "# Constants and variables\n",
    "# ===============================\n",
    "\n",
    "datasets = ['samplesgooglecluster.csv']\n",
    "optimisers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam']\n",
    "rnn_types = ['LSTM']\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "# np.random.seed(0)\n",
    "\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    \"\"\"\n",
    "    Converts an array of values into a dataset matrix\n",
    "    :param dataset:\n",
    "    :param look_back:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back - 1):\n",
    "        a = dataset[i:(i + look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "\n",
    "    collect_gc()\n",
    "\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "def collect_gc():\n",
    "    \"\"\"\n",
    "    Forces garbage collector\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if force_gc:\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def dateparse (time_in_secs):    \n",
    "    return datetime.datetime.fromtimestamp(float(time_in_secs))\n",
    "\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Loads a dataset with training and testing arrays\n",
    "    :param dataset_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    \n",
    "   \n",
    "\n",
    "   \n",
    "    dataset = pd.read_csv(dataset_path, parse_dates=True, usecols=['cpu_usage'])\n",
    "    dataset = dataset.loc[(dataset!=0).any(axis=1)]\n",
    "    print(dataset)\n",
    "    #dataset = pd.to_datetime(dataset)\n",
    "    dataset = dataset.values  # as numpy array\n",
    "    dataset = dataset.astype('float64')\n",
    "    # Normalise the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "    # split into train and test sets\n",
    "    train_size = int(len(dataset) * train_size_percentage)\n",
    "    train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
    "    # reshape into X=t and Y=t+1\n",
    "    train_x, train_y = create_dataset(train, look_back)\n",
    "    test_x, test_y = create_dataset(test, look_back)\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    train_x_stf = np.reshape(train_x, (train_x.shape[0], 1, train_x.shape[1]))\n",
    "    test_x_stf = np.reshape(test_x, (test_x.shape[0], 1, test_x.shape[1]))\n",
    "    train_x_st = np.reshape(train_x, (train_x.shape[0], 1))\n",
    "    test_x_st = np.reshape(test_x, (test_x.shape[0], 1))\n",
    "\n",
    "    return dataset, scaler, train_x_stf, train_x_st, train_y, test_x_stf, test_x_st, test_y\n",
    "\n",
    "\n",
    "def generate_rnn(hidden_layers):\n",
    "    \"\"\"\n",
    "    Generates a RNN using an array of hidden layers including the number of neurons for each layer\n",
    "    :param hidden_layers:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Create and fit the RNN\n",
    "    model = Sequential()\n",
    "    # Add input layer\n",
    "    model.add(Dense(8, input_shape=(1, look_back)))\n",
    "\n",
    "    # Add hidden layers\n",
    "    for i in range(len(hidden_layers)):\n",
    "        neurons_layer = hidden_layers[i]\n",
    "        # Randomly select rnn type of layer\n",
    "        rnn_type_index = random.randint(0, len(rnn_types) - 1)\n",
    "        rnn_type = rnn_types[rnn_type_index]\n",
    "\n",
    "        dropout = random.uniform(0, max_dropout)  # dropout between 0 and max_dropout\n",
    "        return_sequences = i < len(hidden_layers) - 1  # Last layer cannot return sequences when stacking\n",
    "\n",
    "        # Select and add type of layer\n",
    "        if rnn_type == 'LSTM':\n",
    "            model.add(LSTM(neurons_layer, dropout=dropout, return_sequences=return_sequences))\n",
    "        elif rnn_type == 'GRU':\n",
    "            model.add(GRU(neurons_layer, dropout=dropout, return_sequences=return_sequences))\n",
    "        elif rnn_type == 'SimpleRNN':\n",
    "            model.add(SimpleRNN(neurons_layer, dropout=dropout, return_sequences=return_sequences))\n",
    "\n",
    "    collect_gc()\n",
    "\n",
    "    # Add output layer\n",
    "    model.add(Dense(1))\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_rnn(model, train_x, test_x, train_y, test_y, scaler, optimiser):\n",
    "    \"\"\"\n",
    "    Evaluates the RNN model using the training and testing data\n",
    "    :param model:\n",
    "    :param train_x:\n",
    "    :param test_x:\n",
    "    :param train_y:\n",
    "    :param test_y:\n",
    "    :param scaler:\n",
    "    :param optimiser:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimiser)\n",
    "    model.fit(train_x, train_y, epochs=rnn_epochs, batch_size=1, verbose=2)\n",
    "    # Forecast\n",
    "    train_x = [train_x]\n",
    "    test_x = [test_x]\n",
    "    \n",
    "    train_predict = model.predict(train_x)\n",
    "    test_predict = model.predict(test_x)\n",
    "    # Invert forecasts\n",
    "    #print (train_predict)\n",
    "    #print (test_predict)\n",
    "    #train_predict=train_predict.reshape(-1,1)\n",
    "    #test_predict=test_predict.reshape(-1,1)\n",
    "    #train_predict = [train_predict]\n",
    "    #test_predict = [test_predict]\n",
    "    predictions = cross_val_predict(model, df, y, cv=6)\n",
    "    train_predict = scaler.inverse_transform(train_predict)\n",
    "    train_y = scaler.inverse_transform([train_y])\n",
    "    test_predict = scaler.inverse_transform(test_predict)\n",
    "    test_y = scaler.inverse_transform([test_y])\n",
    "    # Calculate RMSE for train and test\n",
    "    train_score = math.sqrt(mean_squared_error(train_y[0], train_predict[:, 0]))\n",
    "    # print('Train Score: %.2f RMSE' % (train_score))\n",
    "    test_score = math.sqrt(mean_squared_error(test_y[0], test_predict[:, 0]))\n",
    "    # print('Test Score: %.2f RMSE' % (test_score))\n",
    "    model.train_score = train_score\n",
    "    model.test_score = test_score\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('rnn evaluated')\n",
    "\n",
    "    return train_score, test_score, train_predict, test_predict, train_predict, test_predict\n",
    "\n",
    "\n",
    "def crossover_rnn(model_1, model_2):\n",
    "    \"\"\"\n",
    "    Executes crossover for the RNN in the GA for 2 models, modifying the first model\n",
    "    :param model_1:\n",
    "    :param model_2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # new_model = copy.copy(model_1)\n",
    "    new_model = model_1\n",
    "\n",
    "    # Probabilty of models depending on their RMSE test score\n",
    "    # Lower RMSE score has higher prob\n",
    "    test_score_total = model_1.test_score + model_2.test_score\n",
    "    model_1_prob = 1 - (model_1.test_score / test_score_total)\n",
    "    model_2_prob = 1 - model_1_prob\n",
    "    # Probabilities of each item for each model (all items have same probabilities)\n",
    "    model_1_prob_item = model_1_prob / (len(model_1.layers) - 2)\n",
    "    model_2_prob_item = model_2_prob / (len(model_2.layers) - 2)\n",
    "\n",
    "    # Number of layers of new generation depend on probability of each model\n",
    "    num_layers_new_gen = int(model_1_prob * (len(model_1.layers) - 1) + model_2_prob * (len(model_2.layers) - 1))\n",
    "\n",
    "    # Create list of int with positions of the layers of both models.\n",
    "    cross_layers_pos = []\n",
    "    # Create list of weights\n",
    "    weights = []\n",
    "    # Add positions of layers for model 1. Input and ouput layer are not added.\n",
    "    for i in range(2, len(model_1.layers)):\n",
    "        mod_item = type('', (), {})()\n",
    "        mod_item.pos = i\n",
    "        mod_item.model = 1\n",
    "        cross_layers_pos.append(mod_item)\n",
    "        weights.append(model_1_prob_item)\n",
    "\n",
    "    # Add positions of layers for model 2. Input and ouput layer are not added.\n",
    "    for i in range(2, len(model_2.layers)):\n",
    "        mod_item = type('', (), {})()\n",
    "        mod_item.pos = i\n",
    "        mod_item.model = 2\n",
    "        cross_layers_pos.append(mod_item)\n",
    "        weights.append(model_2_prob_item)\n",
    "\n",
    "    collect_gc()\n",
    "\n",
    "    # If new num of layers are larger than the num crossover layers, keep num of crossover layers\n",
    "    if num_layers_new_gen > len(cross_layers_pos):\n",
    "        num_layers_new_gen = len(cross_layers_pos)\n",
    "\n",
    "    # Randomly choose num_layers_new_gen layers of the new list\n",
    "    cross_layers_pos = list(np.random.choice(cross_layers_pos, size=num_layers_new_gen, replace=False, p=weights))\n",
    "\n",
    "    # Add both group of hidden layers to new group of layers using previously chosen layer positions of models\n",
    "    cross_layers = []\n",
    "    for i in range(len(cross_layers_pos)):\n",
    "        mod_item = cross_layers_pos[i]\n",
    "        if mod_item.model == 1:\n",
    "            cross_layers.append(model_1.layers[mod_item.pos])\n",
    "        else:\n",
    "            cross_layers.append(model_2.layers[mod_item.pos])\n",
    "\n",
    "    collect_gc()\n",
    "\n",
    "    # Add input layer randomly from parent 1 or parent 2\n",
    "    bit_random = random.randint(0, 1)\n",
    "    if bit_random == 0:\n",
    "        cross_layers.insert(0, model_1.layers[0])\n",
    "    else:\n",
    "        cross_layers.insert(0, model_2.layers[0])\n",
    "\n",
    "    bit_random = random.randint(0, 1)\n",
    "    if bit_random == 0:\n",
    "        cross_layers.append(model_1.layers[len(model_1.layers) - 1])\n",
    "    else:\n",
    "        cross_layers.append(model_2.layers[len(model_2.layers) - 1])\n",
    "\n",
    "    # Set new layers\n",
    "    new_model.layers = cross_layers\n",
    "\n",
    "    return new_model\n",
    "\n",
    "\n",
    "def mutate_rnn(model):\n",
    "    \"\"\"\n",
    "    Mutates the RNN model\n",
    "    :param model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for i in range(len(model.layers)):\n",
    "        # Mutate randomly each layer\n",
    "        bit_random = random.uniform(0, 1)\n",
    "\n",
    "        if bit_random <= mutation_rate:\n",
    "            weights = model.layers[i].get_weights()  # list of weights as numpy arrays\n",
    "            # calculate mutation momentum\n",
    "            mutation_momentum = random.uniform(min_mutation_momentum, max_mutation_momentum)\n",
    "            new_weights = [x * mutation_momentum for x in weights]\n",
    "            model.layers[i].set_weights(new_weights)\n",
    "\n",
    "    collect_gc()\n",
    "\n",
    "\n",
    "def save_plot_model_rnn(model):\n",
    "    \"\"\"\n",
    "    Saves the plot of the RNN model\n",
    "    :param model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"estimator = model.estimators_[0]\n",
    "    out_file = open(\"trees/tree-\" + 0 + \".dot\", 'w')\n",
    "    export_graphviz(estimator, out_file=out_file)\n",
    "    out_file.close()\"\"\"\n",
    "        \n",
    "    #plot_model(model, show_shapes=True)\n",
    "\n",
    "\n",
    "def generate_rf(estimators):\n",
    "    \"\"\"\n",
    "    Generates a Random Forest with the number of estimators to use\n",
    "    :param estimators:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Create and fit the RF\n",
    "    model = RandomForestRegressor(n_estimators=estimators, criterion='mse', max_depth=None, min_samples_split=2,\n",
    "                                  min_samples_leaf=1, max_features='auto', max_leaf_nodes=None, bootstrap=True,\n",
    "                                  oob_score=False, n_jobs=1, random_state=None, verbose=0)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_rf(model, train_x, test_x, train_y, test_y, scaler):\n",
    "    \"\"\"\n",
    "    Evaluates the Random Forest with training and testing data\n",
    "    :param model:\n",
    "    :param train_x:\n",
    "    :param test_x:\n",
    "    :param train_y:\n",
    "    :param test_y:\n",
    "    :param scaler:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model.fit(train_x, train_y)\n",
    "    # Forecast\n",
    "    #train_x =[train_x]\n",
    "    \n",
    "    #test_x = [test_x]\n",
    "    print(len(train_x))\n",
    "    print(len(test_x))\n",
    "    train_predict = model.predict(train_x)\n",
    "    test_predict = model.predict(test_x)\n",
    "    # Invert forecasts\n",
    "    #train_predict.reshape(-1,1)\n",
    "    #test_predict.reshape(-1,1)\n",
    "    train_predict = [train_predict]\n",
    "    test_predict = [test_predict]\n",
    "    #train_y =[train_y]\n",
    "    #test_y = [test_y]\n",
    "    train_predict = cross_val_predict(model, df, y, cv=6)\n",
    "    test_predict = cross_val_predict(model, df, y, cv=6)\n",
    "    train_predict = scaler.inverse_transform(train_predict)\n",
    "    train_y = scaler.inverse_transform([train_y])\n",
    "    test_predict = scaler.inverse_transform(test_predict)\n",
    "    test_y = scaler.inverse_transform([test_y])\n",
    "    # Calculate RMSE for train and test\n",
    "    train_score = math.sqrt(mean_squared_error([train_y[0]], train_predict[:]))\n",
    "    # print('Train Score: %.2f RMSE' % (train_score))\n",
    "    test_score = math.sqrt(mean_squared_error([test_y[0]], test_predict[:]))\n",
    "    # print('Test Score: %.2f RMSE' % (test_score))\n",
    "    model.train_score = train_score\n",
    "    model.test_score = test_score\n",
    "    \n",
    "    \n",
    "\n",
    "    return train_score, test_score, train_predict, test_predict\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def crossover_rf(model_1, model_2):\n",
    "    \"\"\"\n",
    "    Executes crossover for the RF in the GA for 2 models, modifying the first model\n",
    "    :param model_1:\n",
    "    :param model_2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # new_model = copy.copy(model_1)\n",
    "    new_model = model_1\n",
    "\n",
    "    # Probabilty of models depending on their RMSE test score\n",
    "    test_score_total = model_1.test_score + model_2.test_score\n",
    "    model_1_prob = 1 - model_1.test_score / test_score_total\n",
    "    model_2_prob = 1 - model_1_prob\n",
    "\n",
    "    # New estimator is the sum of both estimators times their probability\n",
    "    new_model.n_estimators = math.ceil(model_1.n_estimators * model_1_prob + model_2.n_estimators * model_2_prob)\n",
    "\n",
    "    return new_model\n",
    "\n",
    "\n",
    "def mutate_rf(model):\n",
    "    \"\"\"\n",
    "    Mutates the Random Forest\n",
    "    :param model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Mutate randomly the estimator\n",
    "    bit_random = random.uniform(0, 1)\n",
    "\n",
    "    if bit_random <= mutation_rate:\n",
    "        # calculate mutation momentum\n",
    "        mutation_momentum = random.uniform(min_mutation_momentum, max_mutation_momentum)\n",
    "        # Mutate estimators\n",
    "        model.n_estimators = model.n_estimators + math.ceil(model.n_estimators * mutation_momentum)\n",
    "\n",
    "\n",
    "def save_plot_model_rf(model):\n",
    "    \"\"\"\n",
    "    Saves the plot of the Random Forest model\n",
    "    :param model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \"\"\"for i in range(len(model.estimators_)):\n",
    "        estimator = model.estimators_[i]\n",
    "        out_file = open(\"Results/trees/tree-\" + str(i) + \".dot\", 'w')\n",
    "        export_graphviz(estimator, out_file=out_file)\n",
    "        out_file.close()\"\"\"\n",
    "\n",
    "\n",
    "def ensemble_stacking(model_1_values, model_2_values, test, scaler):\n",
    "    \"\"\"\n",
    "    Ensemble result of 2 models us\n",
    "    ing stacking and averaging.\n",
    "    Takes both model predictions, averages them and calculates the new RMSE\n",
    "    :param model_1_values:\n",
    "    :param model_2_values:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Generates the stacking values by averaging both predictions\n",
    "    stacking_values = []\n",
    "    for i in range(len(model_1_values)):\n",
    "        #print ('model_1_values[i]')\n",
    "        #print (model_1_values[i])\n",
    "        #print ('model_2_values[i]')\n",
    "        #print (model_2_values[0][i])\n",
    "        stacking_values.append((model_1_values[i] + model_2_values[0][i]) / 2)\n",
    "\n",
    "    test = scaler.inverse_transform([test])\n",
    "    rmse = math.sqrt(mean_squared_error(test[0], stacking_values))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.plot( test[0], label='Actual', color='blue')\n",
    "    ax.plot(stacking_values, label='Predicted',color = 'red')\n",
    "    \n",
    "    #plt.xlim(min(dataset['time']), max(dataset['time']))\n",
    "    plt.title('CPU Load Forecast - RNN-RF Ensemble')\n",
    "    plt.ylabel('CPU Load')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return stacking_values, rmse, test\n",
    "\n",
    "\n",
    "def evaluate_ga(dataset):\n",
    "    \"\"\"\n",
    "    Evaluates and generates the ensemble model using Genetic Algorithms\n",
    "    :param dataset:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print('#-----------------------------------------------')\n",
    "    print('  ', dataset)\n",
    "    print('#-----------------------------------------------')\n",
    "\n",
    "    dataset, scaler, train_x_stf, train_x_st, train_y, test_x_stf, test_x_st, test_y = load_dataset(dataset)\n",
    "    start = time.clock()  # Start Timer\n",
    "    num_population = random.randint(min_population, max_population)  # Number of RNN to evaluate\n",
    "    # == 1) Generate initial population for RNN and Random Forest\n",
    "    population_rnn = []\n",
    "    population_rf = []\n",
    "    start_ga_1 = time.clock()  # Start Timer\n",
    "    for i in range(num_population):\n",
    "        # -- RNN\n",
    "        # Generate random topology configuration\n",
    "        num_layers = random.randint(min_num_layers, max_num_layers)\n",
    "        hidden_layers = []\n",
    "        for j in range(num_layers):\n",
    "            num_neurons = random.randint(min_num_neurons, max_num_neurons)\n",
    "            hidden_layers.append(num_neurons)\n",
    "\n",
    "        collect_gc()\n",
    "\n",
    "        # Generate and add rnn model to population\n",
    "        model_rnn = generate_rnn(hidden_layers)\n",
    "        population_rnn.append(model_rnn)\n",
    "\n",
    "        # -- RF\n",
    "        # Generate random number of estimators for RF\n",
    "        num_estimators = random.randint(min_num_estimators, max_num_estimators)\n",
    "\n",
    "        # Generate and add rf model to population\n",
    "        model_rf = generate_rf(num_estimators)\n",
    "        population_rf.append(model_rf)\n",
    "\n",
    "    end_ga_1 = time.clock() - start_ga_1  # End Timer\n",
    "    print('Generate Initial population Time_Taken:%.3f' % end_ga_1)\n",
    "\n",
    "    collect_gc()\n",
    "    # print(len(population))\n",
    "\n",
    "    best_rmse_rnn = float(\"inf\")\n",
    "    best_rmse_rf = float(\"inf\")\n",
    "    best_rnn_model = None\n",
    "    best_test_predict_rnn = None\n",
    "    best_rf_model = None\n",
    "    best_test_predict_rf = None\n",
    "    # Evaluate fitness for\n",
    "    for i in range(num_Iterations):\n",
    "        print('=================================================================================================')\n",
    "        print(' iteration: %d, total iterations: %d, population size: %d ' % (i + 1, num_Iterations, num_population))\n",
    "        print('=================================================================================================')\n",
    "        # train_score, test_score = float(\"inf\"), float(\"inf\")\n",
    "        # == 2)  Evaluate fitness for population\n",
    "        start_ga_2 = time.clock()  # Start Timer\n",
    "        for j in range(num_population):\n",
    "            # Evaluate fitness for RNN\n",
    "            rnn_model = population_rnn[j]\n",
    "            train_score_rnn, test_score_rnn, train_predict_rnn, test_predict_rnn, train_predict, test_predict  = evaluate_rnn(rnn_model, train_x_stf,\n",
    "                                                                                                test_x_stf, train_y,\n",
    "                                                                                                test_y, scaler,\n",
    "                                                                                                optimisers[0])\n",
    "            # print('test predictions RNN: ', test_predict_rnn)\n",
    "            print('test_score RMSE RNN:%.3f ' % test_score_rnn)\n",
    "\n",
    "            if test_score_rnn < best_rmse_rnn:\n",
    "                best_rmse_rnn = test_score_rnn\n",
    "                # best_rnn_model = copy.copy(rnn_model)\n",
    "                best_rnn_model = rnn_model\n",
    "                best_test_predict_rnn = test_predict_rnn\n",
    "\n",
    "            # Evaluate fitness for RF\n",
    "            rf_model = population_rf[j]\n",
    "            train_score_rf, test_score_rf, train_predict_rf, test_predict_rf = evaluate_rf(rf_model, train_x_st,\n",
    "                                                                                           test_x_st, train_y, test_y,\n",
    "                                                                                           scaler)\n",
    "            print('test predictions RF: ', test_predict_rf)\n",
    "            print('test_score RMSE RF:%.3f ' % test_score_rf)\n",
    "\n",
    "            if test_score_rf < best_rmse_rf:\n",
    "                best_rmse_rf = test_score_rf\n",
    "                # best_rf_model = copy.copy(rf_model)\n",
    "                best_rf_model = rf_model\n",
    "                best_test_predict_rf = test_predict_rf\n",
    "\n",
    "        end_ga_2 = time.clock() - start_ga_2  # End Timer\n",
    "        print('Evaluate Fitness population Time_Taken:%.3f' % end_ga_2)\n",
    "\n",
    "        collect_gc()\n",
    "\n",
    "        print('Temporal Best RMSE RNN:%.3f' % best_rmse_rnn)\n",
    "        print('Temporal Best predictions: ', [x[0] for x in best_test_predict_rnn])\n",
    "        print('Temporal Best RMSE RF:%.3f' % best_rmse_rf)\n",
    "        print('Temporal Best predictions: ', [x for x in best_test_predict_rf])\n",
    "\n",
    "        # == 3) Create new population with new generations\n",
    "        # Every generation will use the current best RNN and best RF to mate\n",
    "        start_ga_3 = time.clock()  # Start Timer\n",
    "        for pop_index in range(num_population):\n",
    "            # Select parents for mating\n",
    "            # Element at pop_index as parent. This will be replaced with the new generation\n",
    "            rnn_model_1 = population_rnn[pop_index]\n",
    "            rf_model_1 = population_rf[pop_index]\n",
    "            # 2 parent is the best found so far\n",
    "            rnn_model_2 = best_rnn_model\n",
    "            rf_model_2 = best_rf_model\n",
    "\n",
    "            # == 4) Create new generation with crossover\n",
    "            new_rnn_model = crossover_rnn(rnn_model_1, rnn_model_2)\n",
    "            new_rf_model = crossover_rf(rf_model_1, rf_model_2)\n",
    "\n",
    "            # == 5) Mutate new generation\n",
    "            mutate_rnn(new_rnn_model)\n",
    "            mutate_rf(new_rf_model)\n",
    "\n",
    "            # Replace current model in population\n",
    "            population_rnn[pop_index] = new_rnn_model\n",
    "            population_rf[pop_index] = new_rf_model\n",
    "\n",
    "        end_ga_3 = time.clock() - start_ga_3  # End Timer\n",
    "        print('Generate new population Time_Taken:%.3f' % end_ga_3)\n",
    "\n",
    "        collect_gc()\n",
    "\n",
    "    collect_gc()\n",
    "\n",
    "    end = time.clock() - start  # End Timer\n",
    "\n",
    "    print('=============== BEST RNN ===============')\n",
    "    print('Best predictions: ', [x[0] for x in best_test_predict_rnn])\n",
    "    print('Best RMSE:%.3f Time_Taken:%.3f' % (best_rmse_rnn, end))\n",
    "    #save_plot_model_rnn(best_rnn_model)\n",
    "\n",
    "    print('=============== BEST RF ===============')\n",
    "    print('Best predictions: ', [x for x in best_test_predict_rf])\n",
    "    print('Best RMSE:%.3f Time_Taken:%.3f' % (best_rmse_rf, end))\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Ensemble\n",
    "    print('=============== Ensemble ===============')\n",
    "    averaging_values, rmse, test = ensemble_stacking(best_test_predict_rnn, best_test_predict_rf, test_y, scaler)\n",
    "    print('Ensemble averaging_values: ', averaging_values)\n",
    "    print('Ensemble rmse: ', rmse)\n",
    "    \n",
    "    #save_plot_model_rf(best_rf_model)\n",
    "    # print(best_rf_model.get_params(deep=True))\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.plot( test[0], label='Actual')\n",
    "    ax.plot(averaging_values, label='Predicted')\n",
    "    \n",
    "    #plt.xlim(min(dataset['time']), max(dataset['time']))\n",
    "    plt.xlabel('CPU Load')\n",
    "    plt.ylabel('Steps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_bptt(dataset):\n",
    "    \"\"\"\n",
    "    Evaluates and generates a RNN model using BPTT\n",
    "    :param dataset:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print('#-----------------------------------------------')\n",
    "    print('  ', dataset)\n",
    "    print('#-----------------------------------------------')\n",
    "\n",
    "    dataset, scaler, train_x_stf, train_x_st, train_y, test_x_stf, test_x_st, test_y = load_dataset(dataset)\n",
    "    start = time.clock()  # Start Timer\n",
    "\n",
    "    # Generate a 1 hidden layer configuration\n",
    "    hidden_layers = [10]\n",
    "    # Generate and add rnn model to population\n",
    "    model_rnn = generate_rnn(hidden_layers)\n",
    "\n",
    "    train_score_rnn, test_score_rnn, train_predict_rnn, test_predict_rnn, train_predict, test_predict = evaluate_rnn(model_rnn, train_x_stf,\n",
    "                                                                                        test_x_stf, train_y,\n",
    "                                                                                        test_y, scaler,\n",
    "                                                                                        optimisers[0])\n",
    "\n",
    "    end = time.clock() - start  # End Timer\n",
    "\n",
    "    print('Predictions: ', [x[0] for x in test_predict_rnn])\n",
    "    \n",
    "\n",
    "    print('RMSE:%.3f Time_Taken:%.3f' % (test_score_rnn, end))\n",
    "    #save_plot_model_rnn(model_rnn)\n",
    "    #plotsuc(dataset)\n",
    "    # shift train predictions for plotting\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.plot( test[0], label='Actual')\n",
    "    ax.plot(averaging_values, label='Predicted')\n",
    "    \n",
    "    #plt.xlim(min(dataset['time']), max(dataset['time']))\n",
    "    plt.xlabel('CPU Load')\n",
    "    plt.ylabel('Steps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print('success')\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    evaluate_ga(datasets[0])\n",
    "    # evaluate_ga(datasets[1])\n",
    "    # evaluate_ga(datasets[2])\n",
    "\n",
    "    #evaluate_bptt(datasets[0])\n",
    "    # evaluate_bptt(datasets[1])\n",
    "    # evaluate_bptt(datasets[2])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
